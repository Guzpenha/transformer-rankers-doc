

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>transformer_rankers.models.pairwise_bert.BertForPairwiseLearning &mdash; Transformer-Rankers  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="transformer_rankers.models.pointwise_bert" href="transformer_rankers.models.pointwise_bert.html" />
    <link rel="prev" title="transformer_rankers.models.pairwise_bert" href="transformer_rankers.models.pairwise_bert.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> Transformer-Rankers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick-start.html">Quick-start</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../quick-start.html#setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quick-start.html#example-i-supported-dataset">Example (I) - Supported dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quick-start.html#example-ii-custom-dataset">Example (II) - Custom dataset</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../main-modules.html">Intro to Components</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../main-modules.html#datasets">datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../main-modules.html#negative-samplers">negative_samplers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../main-modules.html#eval">eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../main-modules.html#trainers">trainers</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Documentation</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="transformer_rankers.html">transformer_rankers</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="transformer_rankers.datasets.html">transformer_rankers.datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.datasets.dataset.html">transformer_rankers.datasets.dataset</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.dataset.collate_T2T_batch.html">transformer_rankers.datasets.dataset.collate_T2T_batch</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.dataset.AbstractDataloader.html">transformer_rankers.datasets.dataset.AbstractDataloader</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.dataset.QueryDocumentDataLoader.html">transformer_rankers.datasets.dataset.QueryDocumentDataLoader</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.dataset.QueryDocumentDataset.html">transformer_rankers.datasets.dataset.QueryDocumentDataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.dataset.QueryPosDocNegDocDataLoader.html">transformer_rankers.datasets.dataset.QueryPosDocNegDocDataLoader</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.dataset.QueryPosDocNegDocDataset.html">transformer_rankers.datasets.dataset.QueryPosDocNegDocDataset</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.datasets.downloader.html">transformer_rankers.datasets.downloader</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.downloader.download_file_from_google_drive.html">transformer_rankers.datasets.downloader.download_file_from_google_drive</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.downloader.get_confirm_token.html">transformer_rankers.datasets.downloader.get_confirm_token</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.downloader.save_response_content.html">transformer_rankers.datasets.downloader.save_response_content</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.downloader.DataDownloader.html">transformer_rankers.datasets.downloader.DataDownloader</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.datasets.preprocess_crr.html">transformer_rankers.datasets.preprocess_crr</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.preprocess_crr.read_crr_tsv_as_df.html">transformer_rankers.datasets.preprocess_crr.read_crr_tsv_as_df</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.preprocess_crr.transform_dstc8_to_tsv.html">transformer_rankers.datasets.preprocess_crr.transform_dstc8_to_tsv</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.datasets.preprocess_pr.html">transformer_rankers.datasets.preprocess_pr</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.preprocess_pr.transform_trec2020pr_to_dfs.html">transformer_rankers.datasets.preprocess_pr.transform_trec2020pr_to_dfs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.datasets.preprocess_sqr.html">transformer_rankers.datasets.preprocess_sqr</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.preprocess_sqr.transform_linkso_to_duplicates_dfs.html">transformer_rankers.datasets.preprocess_sqr.transform_linkso_to_duplicates_dfs</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.preprocess_sqr.transform_quora_question_pairs_to_duplicates_dfs.html">transformer_rankers.datasets.preprocess_sqr.transform_quora_question_pairs_to_duplicates_dfs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.datasets.processors.html">transformer_rankers.datasets.processors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.processors.clariq_processor.html">transformer_rankers.datasets.processors.clariq_processor</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.processors.linkso_processor.html">transformer_rankers.datasets.processors.linkso_processor</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.processors.mantis_processor.html">transformer_rankers.datasets.processors.mantis_processor</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.processors.msdialog_processor.html">transformer_rankers.datasets.processors.msdialog_processor</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.processors.qqp_processor.html">transformer_rankers.datasets.processors.qqp_processor</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.processors.trec2020pr_processor.html">transformer_rankers.datasets.processors.trec2020pr_processor</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.datasets.processors.ubuntu_dstc8_processor.html">transformer_rankers.datasets.processors.ubuntu_dstc8_processor</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transformer_rankers.eval.html">transformer_rankers.eval</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.eval.evaluation.html">transformer_rankers.eval.evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.eval.evaluation.evaluate_models.html">transformer_rankers.eval.evaluation.evaluate_models</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.eval.evaluation.recall_at_with_k_candidates.html">transformer_rankers.eval.evaluation.recall_at_with_k_candidates</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.eval.results_analyses_tools.html">transformer_rankers.eval.results_analyses_tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.eval.results_analyses_tools.evaluate.html">transformer_rankers.eval.results_analyses_tools.evaluate</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.eval.results_analyses_tools.evaluate_and_aggregate.html">transformer_rankers.eval.results_analyses_tools.evaluate_and_aggregate</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transformer_rankers.examples.html">transformer_rankers.examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.examples.aggregate_sacred_result_logs.html">transformer_rankers.examples.aggregate_sacred_result_logs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.examples.aggregate_sacred_result_logs.main.html">transformer_rankers.examples.aggregate_sacred_result_logs.main</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.examples.bert_ranker_cross_task_cross_ns_for_crr.html">transformer_rankers.examples.bert_ranker_cross_task_cross_ns_for_crr</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.examples.bert_ranker_cross_task_cross_ns_for_crr.main.html">transformer_rankers.examples.bert_ranker_cross_task_cross_ns_for_crr.main</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.examples.bert_ranker_cross_task_cross_ns_for_crr.run_experiment.html">transformer_rankers.examples.bert_ranker_cross_task_cross_ns_for_crr.run_experiment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.examples.download_task_data.html">transformer_rankers.examples.download_task_data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.examples.download_task_data.main.html">transformer_rankers.examples.download_task_data.main</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.examples.negative_sampling.html">transformer_rankers.examples.negative_sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.examples.negative_sampling.main.html">transformer_rankers.examples.negative_sampling.main</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.examples.pairwise_bert_ranker.html">transformer_rankers.examples.pairwise_bert_ranker</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.examples.pairwise_bert_ranker.main.html">transformer_rankers.examples.pairwise_bert_ranker.main</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.examples.pairwise_bert_ranker.run_experiment.html">transformer_rankers.examples.pairwise_bert_ranker.run_experiment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.examples.pointwise_bert_ranker.html">transformer_rankers.examples.pointwise_bert_ranker</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.examples.pointwise_bert_ranker.main.html">transformer_rankers.examples.pointwise_bert_ranker.main</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.examples.pointwise_bert_ranker.run_experiment.html">transformer_rankers.examples.pointwise_bert_ranker.run_experiment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.examples.t5_ranker.html">transformer_rankers.examples.t5_ranker</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.examples.t5_ranker.main.html">transformer_rankers.examples.t5_ranker.main</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.examples.t5_ranker.run_experiment.html">transformer_rankers.examples.t5_ranker.run_experiment</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="transformer_rankers.models.html">transformer_rankers.models</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.models.losses.html">transformer_rankers.models.losses</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.models.losses.label_smoothing.html">transformer_rankers.models.losses.label_smoothing</a><ul>
<li class="toctree-l5"><a class="reference internal" href="transformer_rankers.models.losses.label_smoothing.LabelSmoothingCrossEntropy.html">transformer_rankers.models.losses.label_smoothing.LabelSmoothingCrossEntropy</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3 current"><a class="reference internal" href="transformer_rankers.models.pairwise_bert.html">transformer_rankers.models.pairwise_bert</a><ul class="current">
<li class="toctree-l4 current"><a class="current reference internal" href="#">transformer_rankers.models.pairwise_bert.BertForPairwiseLearning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.models.pointwise_bert.html">transformer_rankers.models.pointwise_bert</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.models.pointwise_bert.BertForPointwiseLearning.html">transformer_rankers.models.pointwise_bert.BertForPointwiseLearning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transformer_rankers.negative_samplers.html">transformer_rankers.negative_samplers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.negative_samplers.negative_sampling.html">transformer_rankers.negative_samplers.negative_sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.negative_samplers.negative_sampling.BM25NegativeSamplerPyserini.html">transformer_rankers.negative_samplers.negative_sampling.BM25NegativeSamplerPyserini</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.negative_samplers.negative_sampling.RandomNegativeSampler.html">transformer_rankers.negative_samplers.negative_sampling.RandomNegativeSampler</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.negative_samplers.negative_sampling.SentenceBERTNegativeSampler.html">transformer_rankers.negative_samplers.negative_sampling.SentenceBERTNegativeSampler</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transformer_rankers.scripts.html">transformer_rankers.scripts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.scripts.select_ns_examples.html">transformer_rankers.scripts.select_ns_examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.scripts.select_ns_examples.main.html">transformer_rankers.scripts.select_ns_examples.main</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.scripts.train_sentenceBERT_crr.html">transformer_rankers.scripts.train_sentenceBERT_crr</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.scripts.train_sentenceBERT_crr.main.html">transformer_rankers.scripts.train_sentenceBERT_crr.main</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.scripts.train_sentenceBERT_crr.CRRBenchmarkDataReader.html">transformer_rankers.scripts.train_sentenceBERT_crr.CRRBenchmarkDataReader</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.scripts.train_sentenceBERT_crr.CRRDataReader.html">transformer_rankers.scripts.train_sentenceBERT_crr.CRRDataReader</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transformer_rankers.trainers.html">transformer_rankers.trainers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.trainers.transformer_trainer.html">transformer_rankers.trainers.transformer_trainer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.trainers.transformer_trainer.TransformerTrainer.html">transformer_rankers.trainers.transformer_trainer.TransformerTrainer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transformer_rankers.utils.html">transformer_rankers.utils</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transformer_rankers.utils.utils.html">transformer_rankers.utils.utils</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.utils.utils.acumulate_l1_by_l2.html">transformer_rankers.utils.utils.acumulate_l1_by_l2</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.utils.utils.acumulate_list.html">transformer_rankers.utils.utils.acumulate_list</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.utils.utils.acumulate_list_multiple_relevant.html">transformer_rankers.utils.utils.acumulate_list_multiple_relevant</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_rankers.utils.utils.from_df_to_list_without_nans.html">transformer_rankers.utils.utils.from_df_to_list_without_nans</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Transformer-Rankers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="transformer_rankers.html">transformer_rankers</a> &raquo;</li>
        
          <li><a href="transformer_rankers.models.html">transformer_rankers.models</a> &raquo;</li>
        
          <li><a href="transformer_rankers.models.pairwise_bert.html">transformer_rankers.models.pairwise_bert</a> &raquo;</li>
        
      <li>transformer_rankers.models.pairwise_bert.BertForPairwiseLearning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="transformer-rankers-models-pairwise-bert-bertforpairwiselearning">
<h1>transformer_rankers.models.pairwise_bert.BertForPairwiseLearning<a class="headerlink" href="#transformer-rankers-models-pairwise-bert-bertforpairwiselearning" title="Permalink to this headline">¶</a></h1>
<dl class="py class">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning">
<em class="property">class </em><code class="sig-prename descclassname">transformer_rankers.models.pairwise_bert.</code><code class="sig-name descname">BertForPairwiseLearning</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em>, <em class="sig-param"><span class="n">loss_function</span><span class="o">=</span><span class="default_value">'cross-entropy'</span></em>, <em class="sig-param"><span class="n">smoothing</span><span class="o">=</span><span class="default_value">0.1</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/transformer_rankers/models/pairwise_bert.py#L6-L68"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertPreTrainedModel</span></code></p>
<p>BERT based model for pairwise learning. It expects both the &lt;q, positive_doc&gt; and the &lt;q, negative_doc&gt;
for doing the forward pass. The loss is cross-entropy for the difference between positive_doc and negative_doc
scores (labels are 1 if score positive_neg &gt; score negative_doc otherwise 0) based on
“Learning to Rank using Gradient Descent” 2005 ICML.</p>
<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em>, <em class="sig-param"><span class="n">loss_function</span><span class="o">=</span><span class="default_value">'cross-entropy'</span></em>, <em class="sig-param"><span class="n">smoothing</span><span class="o">=</span><span class="default_value">0.1</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/transformer_rankers/models/pairwise_bert.py#L13-L27"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
</dd></dl>

<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.__init__" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.__init__"><code class="xref py py-obj docutils literal notranslate"><span class="pre">__init__</span></code></a>(config[, loss_function, smoothing])</p></td>
<td><p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.add_memory_hooks" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.add_memory_hooks"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_memory_hooks</span></code></a>()</p></td>
<td><p>Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.add_module" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.add_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_module</span></code></a>(name, module)</p></td>
<td><p>Adds a child module to the current module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.adjust_logits_during_generation" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.adjust_logits_during_generation"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adjust_logits_during_generation</span></code></a>(logits, **kwargs)</p></td>
<td><p>Implement in subclasses of <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code> for custom behavior to adjust the logits in the generate method.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.apply" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.apply"><code class="xref py py-obj docutils literal notranslate"><span class="pre">apply</span></code></a>(fn)</p></td>
<td><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>) as well as self.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.bfloat16" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.bfloat16"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bfloat16</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.buffers" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.buffers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">buffers</span></code></a>([recurse])</p></td>
<td><p>Returns an iterator over module buffers.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.children" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.children"><code class="xref py py-obj docutils literal notranslate"><span class="pre">children</span></code></a>()</p></td>
<td><p>Returns an iterator over immediate children modules.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.cpu" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.cpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cpu</span></code></a>()</p></td>
<td><p>Moves all model parameters and buffers to the CPU.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.cuda" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.cuda"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cuda</span></code></a>([device])</p></td>
<td><p>Moves all model parameters and buffers to the GPU.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.double" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.double"><code class="xref py py-obj docutils literal notranslate"><span class="pre">double</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.enforce_repetition_penalty_" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.enforce_repetition_penalty_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">enforce_repetition_penalty_</span></code></a>(lprobs, …)</p></td>
<td><p>Enforce the repetition penalty (from the <a class="reference external" href="https://arxiv.org/abs/1909.05858">CTRL paper</a>).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.eval" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eval</span></code></a>()</p></td>
<td><p>Sets the module in evaluation mode.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.extra_repr" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.extra_repr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">extra_repr</span></code></a>()</p></td>
<td><p>Set the extra representation of the module</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.float" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.float"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to float datatype.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.forward" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a>([input_ids_pos, attention_mask_pos, …])</p></td>
<td><p>Defines the computation performed at every call.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.from_pretrained" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.from_pretrained"><code class="xref py py-obj docutils literal notranslate"><span class="pre">from_pretrained</span></code></a>(…)</p></td>
<td><p>Instantiate a pretrained pytorch model from a pre-trained model configuration.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.generate" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.generate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code></a>([input_ids, max_length, …])</p></td>
<td><p>Generates sequences for models with a language modeling head.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_extended_attention_mask" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_extended_attention_mask"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_extended_attention_mask</span></code></a>(attention_mask, …)</p></td>
<td><p>Makes broadcastable attention and causal masks so that future and masked tokens are ignored.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_head_mask" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_head_mask"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_head_mask</span></code></a>(head_mask, num_hidden_layers)</p></td>
<td><p>Prepare the head mask if needed.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_input_embeddings" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_input_embeddings"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_input_embeddings</span></code></a>()</p></td>
<td><p>Returns the model’s input embeddings.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_output_embeddings" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_output_embeddings"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_output_embeddings</span></code></a>()</p></td>
<td><p>Returns the model’s output embeddings.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.half" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.half"><code class="xref py py-obj docutils literal notranslate"><span class="pre">half</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.init_weights" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.init_weights"><code class="xref py py-obj docutils literal notranslate"><span class="pre">init_weights</span></code></a>()</p></td>
<td><p>Initializes and prunes weights if needed.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.invert_attention_mask" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.invert_attention_mask"><code class="xref py py-obj docutils literal notranslate"><span class="pre">invert_attention_mask</span></code></a>(encoder_attention_mask)</p></td>
<td><p>Invert an attention mask (e.g., switches 0.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.load_state_dict" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.load_state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_state_dict</span></code></a>(state_dict[, strict])</p></td>
<td><p>Copies parameters and buffers from <a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.state_dict" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into this module and its descendants.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.load_tf_weights" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.load_tf_weights"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_tf_weights</span></code></a>(config, tf_checkpoint_path)</p></td>
<td><p>Load tf checkpoints in a pytorch model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.modules" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules</span></code></a>()</p></td>
<td><p>Returns an iterator over all modules in the network.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_buffers" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_buffers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_buffers</span></code></a>([prefix, recurse])</p></td>
<td><p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_children" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_children"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_children</span></code></a>()</p></td>
<td><p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_modules" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_modules</span></code></a>([memo, prefix])</p></td>
<td><p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_parameters" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_parameters</span></code></a>([prefix, recurse])</p></td>
<td><p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.num_parameters" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.num_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">num_parameters</span></code></a>([only_trainable])</p></td>
<td><p>Get the number of (optionally, trainable) parameters in the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.parameters" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">parameters</span></code></a>([recurse])</p></td>
<td><p>Returns an iterator over module parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">postprocess_next_token_scores</span></code>(scores, …)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.prepare_inputs_for_generation" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.prepare_inputs_for_generation"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_inputs_for_generation</span></code></a>(input_ids, …)</p></td>
<td><p>Implement in subclasses of <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code> for custom behavior to prepare inputs in the generate method.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.prune_heads" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.prune_heads"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prune_heads</span></code></a>(heads_to_prune)</p></td>
<td><p>Prunes heads of the base model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_backward_hook" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_backward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_backward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a backward hook on the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_buffer" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_buffer</span></code></a>(name, tensor)</p></td>
<td><p>Adds a persistent buffer to the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_forward_hook" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_forward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_forward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a forward hook on the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_forward_pre_hook" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_forward_pre_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code></a>(hook)</p></td>
<td><p>Registers a forward pre-hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_parameter" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_parameter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_parameter</span></code></a>(name, param)</p></td>
<td><p>Adds a parameter to the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.requires_grad_" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.requires_grad_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">requires_grad_</span></code></a>([requires_grad])</p></td>
<td><p>Change if autograd should record operations on parameters in this module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.reset_memory_hooks_state" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.reset_memory_hooks_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reset_memory_hooks_state</span></code></a>()</p></td>
<td><p>Reset the <code class="xref py py-obj docutils literal notranslate"><span class="pre">mem_rss_diff</span></code> attribute of each module (see <code class="xref py py-func docutils literal notranslate"><span class="pre">add_memory_hooks()</span></code>).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.resize_token_embeddings" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.resize_token_embeddings"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resize_token_embeddings</span></code></a>([new_num_tokens])</p></td>
<td><p>Resizes input token embeddings matrix of the model if <code class="xref py py-obj docutils literal notranslate"><span class="pre">new_num_tokens</span> <span class="pre">!=</span> <span class="pre">config.vocab_size</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.save_pretrained" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.save_pretrained"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_pretrained</span></code></a>(save_directory)</p></td>
<td><p>Save a model and its configuration file to a directory, so that it can be re-loaded using the <cite>:func:`~transformers.PreTrainedModel.from_pretrained`</cite> class method.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.set_input_embeddings" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.set_input_embeddings"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_input_embeddings</span></code></a>(value)</p></td>
<td><p>Set model’s input embeddings</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">share_memory</span></code>()</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.state_dict" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_dict</span></code></a>([destination, prefix, keep_vars])</p></td>
<td><p>Returns a dictionary containing a whole state of the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.tie_weights" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.tie_weights"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tie_weights</span></code></a>()</p></td>
<td><p>Tie the weights between the input embeddings and the output embeddings.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#id3" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to</span></code></a>(*args, **kwargs)</p></td>
<td><p>Moves and/or casts the parameters and buffers.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.train" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train</span></code></a>([mode])</p></td>
<td><p>Sets the module in training mode.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.type" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">type</span></code></a>(dst_type)</p></td>
<td><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.zero_grad" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zero_grad</span></code></a>()</p></td>
<td><p>Sets gradients of all model parameters to zero.</p></td>
</tr>
</tbody>
</table>
<p class="rubric">Attributes</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">authorized_missing_keys</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.base_model" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.base_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">base_model</span></code></a></p></td>
<td><p>The main body of the model.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">base_model_prefix</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.device" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">device</span></code></a></p></td>
<td><p>The device on which the module is (assuming that all the module parameters are on the same device).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.dtype" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.dtype"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dtype</span></code></a></p></td>
<td><p>The dtype of the module (assuming that all the module parameters have the same dtype).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.dummy_inputs" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.dummy_inputs"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dummy_inputs</span></code></a></p></td>
<td><p>Dummy inputs to do a forward pass in the network.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">dump_patches</span></code></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.add_memory_hooks">
<code class="sig-name descname">add_memory_hooks</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_utils.py#L134-L144"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.add_memory_hooks" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.</p>
<p>Increase in memory consumption is stored in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">mem_rss_diff</span></code> attribute for each module and can be reset to
zero with <code class="xref py py-obj docutils literal notranslate"><span class="pre">model.reset_memory_hooks_state()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.add_module">
<code class="sig-name descname">add_module</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">module</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L177-L199"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.add_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the child module. The child module can be
accessed from this module using the given name</p></li>
<li><p><strong>module</strong> (<em>Module</em>) – child module to be added to the module.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.adjust_logits_during_generation">
<code class="sig-name descname">adjust_logits_during_generation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">logits</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/generation_utils.py#L43-L48"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.adjust_logits_during_generation" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement in subclasses of <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code> for custom behavior to adjust the logits in
the generate method.</p>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.apply">
<code class="sig-name descname">apply</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L250-L291"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>)
as well as self. Typical use includes initializing the parameters of a model
(see also <span class="xref std std-ref">nn-init-doc</span>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> -&gt; None) – function to be applied to each submodule</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.base_model">
<em class="property">property </em><code class="sig-name descname">base_model</code><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/transformer_rankers/models/pairwise_bert.py"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>The main body of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.bfloat16">
<code class="sig-name descname">bfloat16</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L352-L358"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.bfloat16" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.buffers">
<code class="sig-name descname">buffers</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">recurse</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L913-L933"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>torch.Tensor</em> – module buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">buf</span><span class="p">),</span> <span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.children">
<code class="sig-name descname">children</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L961-L968"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a child module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.config_class">
<code class="sig-name descname">config_class</code><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/configuration_bert.py#L51-L141"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.configuration_bert.BertConfig</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.cpu">
<code class="sig-name descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L309-L315"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.cuda">
<code class="sig-name descname">cuda</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">device</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L293-L307"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>int</em><em>, </em><em>optional</em>) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.device">
<em class="property">property </em><code class="sig-name descname">device</code><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/transformer_rankers/models/pairwise_bert.py"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.device" title="Permalink to this definition">¶</a></dt>
<dd><p>The device on which the module is (assuming that all the module parameters are on the same
device).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.device</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.double">
<code class="sig-name descname">double</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L336-L342"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.dtype">
<em class="property">property </em><code class="sig-name descname">dtype</code><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/transformer_rankers/models/pairwise_bert.py"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>The dtype of the module (assuming that all the module parameters have the same dtype).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.dtype</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.dummy_inputs">
<em class="property">property </em><code class="sig-name descname">dummy_inputs</code><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/transformer_rankers/models/pairwise_bert.py"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.dummy_inputs" title="Permalink to this definition">¶</a></dt>
<dd><p>Dummy inputs to do a forward pass in the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.enforce_repetition_penalty_">
<code class="sig-name descname">enforce_repetition_penalty_</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">lprobs</span></em>, <em class="sig-param"><span class="n">batch_size</span></em>, <em class="sig-param"><span class="n">num_beams</span></em>, <em class="sig-param"><span class="n">prev_output_tokens</span></em>, <em class="sig-param"><span class="n">repetition_penalty</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/generation_utils.py#L50-L60"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.enforce_repetition_penalty_" title="Permalink to this definition">¶</a></dt>
<dd><p>Enforce the repetition penalty (from the <a class="reference external" href="https://arxiv.org/abs/1909.05858">CTRL paper</a>).</p>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.eval">
<code class="sig-name descname">eval</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L1075-L1088"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<p>This is equivalent with <code class="xref py py-meth docutils literal notranslate"><span class="pre">self.train(False)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.extra_repr">
<code class="sig-name descname">extra_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L1124-L1131"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should reimplement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.float">
<code class="sig-name descname">float</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L328-L334"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to float datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids_pos</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask_pos</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">token_type_ids_pos</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">input_ids_neg</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask_neg</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">token_type_ids_neg</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">labels</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/transformer_rankers/models/pairwise_bert.py#L29-L68"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.from_pretrained">
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_utils.py#L654-L995"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiate a pretrained pytorch model from a pre-trained model configuration.</p>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (Dropout modules are deactivated).
To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code>.</p>
<p>The warning <cite>Weights from XXX not initialized from pretrained model</cite> means that the weights of XXX do not come
pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
task.</p>
<p>The warning <cite>Weights from XXX not used in YYY</cite> means that the layer XXX is not used by YYY, therefore those
weights are discarded.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string with the <cite>shortcut name</cite> of a pretrained model to load from cache or download, e.g.,
<code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</p></li>
<li><p>A string with the <cite>identifier name</cite> of a pretrained model that was user-uploaded to our S3, e.g.,
<code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>tensorflow index checkpoint file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./tf_model/model.ckpt.index</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if you are both providing the configuration and state dictionary (resp. with keyword
arguments <code class="docutils literal notranslate"><span class="pre">config</span></code> and <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>).</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (sequence of positional arguments, <cite>optional</cite>) – All remaning positional arguments will be passed to the underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p></li>
<li><p><strong>config</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[PretrainedConfig,</span> <span class="pre">str]</span></code>, <cite>optional</cite>) – <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>an instance of a class derived from <code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code>,</p></li>
<li><p>a string valid as input to <code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code>.</p></li>
</ul>
</div></blockquote>
<p>Configuration for the model to use instead of an automatically loaded configuation. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>shortcut name</cite> string of a
pretrained model).</p></li>
<li><p>The model was saved using <code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code> and is reloaded
by suppling the save directory.</p></li>
<li><p>The model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>, <cite>optional</cite>) – <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code> and
<code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) – A dictionary of proxy servers to use by protocol or endpoint, e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span> <span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each
request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether ot not to also return a dictionnary containing missing keys, unexpected keys and error
messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether or not to only look at local files (e.g., not try doanloading the model).</p></li>
<li><p><strong>use_cdn</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – Whether or not to use Cloudfront (a Content Delivery Network, or CDN) when searching for the model on
our S3 (faster). Should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> for checkpoints larger than 20GB.</p></li>
<li><p><strong>kwargs</strong> (remaining dictionary of keyword arguments, <cite>optional</cite>) – <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attention=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertConfig</span><span class="p">,</span> <span class="n">BertModel</span>
<span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="c1"># Model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)` (for example purposes, not runnable).</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./test/saved_model/&#39;</span><span class="p">)</span>
<span class="c1"># Update configuration during loading.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span> <span class="n">output_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">&#39;./tf_model/my_tf_model_config.json&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./tf_model/my_tf_checkpoint.ckpt.index&#39;</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.generate">
<code class="sig-name descname">generate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.LongTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">min_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">do_sample</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">early_stopping</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_beams</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">temperature</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">top_k</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">top_p</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">repetition_penalty</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">bad_words_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Iterable<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">bos_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">length_penalty</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">no_repeat_ngram_size</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_return_sequences</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.LongTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">decoder_start_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">model_kwargs</span></em><span class="sig-paren">)</span> &#x2192; torch.LongTensor<a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/autograd/grad_mode.py#L110-L497"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.generate" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates sequences for models with a language modeling head. The method currently supports greedy decoding,
beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.</p>
<p>Adapted in part from <a class="reference external" href="https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529">Facebook’s XLM beam search code</a>.</p>
<p>Apart from <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">attention_mask</span></code>, all the arguments below will default to the value of the
attribute of the same name inside the <code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code> of the model. The default values
indicated are the default values of those config.</p>
<p>Most of these parameters are explained in more detail in <a class="reference external" href="https://huggingface.co/blog/how-to-generate">this blog post</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – The sequence used as a prompt for the generation. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> the method initializes
it as an empty <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>.</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 20) – The maximum length of the sequence to be generated.</p></li>
<li><p><strong>min_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 10) – The minimum length of the sequence to be generated.</p></li>
<li><p><strong>do_sample</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether or not to use sampling ; use greedy decoding otherwise.</p></li>
<li><p><strong>early_stopping</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to stop the beam search when at least <code class="docutils literal notranslate"><span class="pre">num_beams</span></code> sentences are finished per batch or not.</p></li>
<li><p><strong>num_beams</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) – Number of beams for beam search. 1 means no beam search.</p></li>
<li><p><strong>temperature</strong> (<a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.float" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.float"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code></a>, <cite>optional</cite>, defaults tp 1.0) – The value used to module the next token probabilities.</p></li>
<li><p><strong>top_k</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 50) – The number of highest probability vocabulary tokens to keep for top-k-filtering.</p></li>
<li><p><strong>top_p</strong> (<a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.float" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.float"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code></a>, <cite>optional</cite>, defaults to 1.0) – If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code class="docutils literal notranslate"><span class="pre">top_p</span></code> or
higher are kept for generation.</p></li>
<li><p><strong>repetition_penalty</strong> (<a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.float" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.float"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code></a>, <cite>optional</cite>, defaults to 1.0) – The parameter for repetition penalty. 1.0 means no penalty. See <a class="reference external" href="https://arxiv.org/pdf/1909.05858.pdf">this paper</a> for more details.</p></li>
<li><p><strong>pad_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – The id of the <cite>padding</cite> token.</p></li>
<li><p><strong>bos_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – The id of the <cite>beginning-of-sequence</cite> token.</p></li>
<li><p><strong>eos_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – The id of the <cite>end-of-sequence</cite> token.</p></li>
<li><p><strong>length_penalty</strong> (<a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.float" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.float"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code></a>, <cite>optional</cite>, defaults to 1.0) – <p>Exponential penalty to the length. 1.0 means no penalty.</p>
<p>Set to values &lt; 1.0 in order to encourage the model to generate shorter sequences, to a value &gt; 1.0 in
order to encourage the model to produce longer sequences.</p>
</p></li>
<li><p><strong>no_repeat_ngram_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) – If set to int &gt; 0, all ngrams of that size can only occur once.</p></li>
<li><p><strong>bad_words_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>) – List of token ids that are not allowed to be generated. In order to get the tokens of the words that
should not appear in the generated text, use <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.encode(bad_word,</span> <span class="pre">add_prefix_space=True)</span></code>.</p></li>
<li><p><strong>num_return_sequences</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) – The number of independently computed returned sequences for each element in the batch.</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Mask to avoid performing attention on padding token indices. Mask values are in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>, 1 for
tokens that are not masked, and 0 for masked tokens.</p>
<p>If not provided, will default to a tensor the same shape as <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> that masks the pad token.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>decoder_start_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – If an encoder-decoder model starts decoding with a different token than <cite>bos</cite>, the id of that token.</p></li>
<li><p><strong>use_cache</strong> – (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):
Whether or not the model should use the past last key/values attentions (if applicable to the model) to
speed up decoding.</p></li>
<li><p><strong>model_kwargs</strong> – Additional model specific kwargs will be forwarded to the <a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.forward" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a> function of the model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The generated sequences. The second dimension (sequence_length) is either equal to <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
shorter if all batches finished early due to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">eos_token_id</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size</span> <span class="pre">*</span> <span class="pre">num_return_sequences,</span> <span class="pre">sequence_length)</span></code></p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilgpt2&#39;</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilgpt2&#39;</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>  <span class="c1"># do greedy decoding</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Generated: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;openai-gpt&#39;</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;openai-gpt&#39;</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">input_context</span> <span class="o">=</span> <span class="s1">&#39;The dog&#39;</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>  <span class="c1"># encode input context</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>  <span class="c1"># generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context &#39;The dog&#39;</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="c1">#  3 output sequences were generated</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Generated </span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilgpt2&#39;</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilgpt2&#39;</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">input_context</span> <span class="o">=</span> <span class="s1">&#39;The dog&#39;</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>  <span class="c1"># encode input context</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># generate 3 candidates using sampling</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="c1">#  3 output sequences were generated</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Generated </span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;ctrl&#39;</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;ctrl&#39;</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">input_context</span> <span class="o">=</span> <span class="s1">&#39;Legal My neighbor is&#39;</span>  <span class="c1"># &quot;Legal&quot; is one of the control codes for ctrl</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>  <span class="c1"># encode input context</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>  <span class="c1"># generate sequences</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Generated: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">input_context</span> <span class="o">=</span> <span class="s1">&#39;My cute dog&#39;</span>  <span class="c1"># &quot;Legal&quot; is one of the control codes for ctrl</span>
<span class="n">bad_words_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">bad_word</span><span class="p">,</span> <span class="n">add_prefix_space</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">bad_word</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;idiot&#39;</span><span class="p">,</span> <span class="s1">&#39;stupid&#39;</span><span class="p">,</span> <span class="s1">&#39;shut up&#39;</span><span class="p">]]</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>  <span class="c1"># encode input context</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bad_words_ids</span><span class="o">=</span><span class="n">bad_words_ids</span><span class="p">)</span>  <span class="c1"># generate sequences without allowing bad_words to be generated</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_extended_attention_mask">
<code class="sig-name descname">get_extended_attention_mask</code><span class="sig-paren">(</span><em class="sig-param">attention_mask: torch.Tensor, input_shape: Tuple[int], device: &lt;property object at 0x7ff2538d2c78&gt;</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_utils.py#L227-L273"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_extended_attention_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes broadcastable attention and causal masks so that future and masked tokens are ignored.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – Mask with ones indicating tokens to attend to, zeros for tokens to ignore.</p></li>
<li><p><strong>input_shape</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[int]</span></code>) – The shape of the input to the model.</p></li>
<li><p><strong>device</strong> – (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.device</span></code>):
The device of the input to the model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> The extended attention mask, with a the same dtype as <code class="xref py py-obj docutils literal notranslate"><span class="pre">attention_mask.dtype</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_head_mask">
<code class="sig-name descname">get_head_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">head_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">num_hidden_layers</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">is_attention_chunked</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_utils.py#L275-L300"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_head_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare the head mask if needed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> with shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">[num_heads]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">[num_hidden_layers</span> <span class="pre">x</span> <span class="pre">num_heads]</span></code>, <cite>optional</cite>) – The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard).</p></li>
<li><p><strong>num_hidden_layers</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – The number of hidden layers in the model.</p></li>
<li><p><strong>is_attention_chunked</strong> – (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional, defaults to :obj:`False</cite>):
Whether or not the attentions scores are computed by chunks or not.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> with shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">[num_hidden_layers</span> <span class="pre">x</span> <span class="pre">batch</span> <span class="pre">x</span> <span class="pre">num_heads</span> <span class="pre">x</span> <span class="pre">seq_length</span> <span class="pre">x</span> <span class="pre">seq_length]</span></code>
or list with <code class="xref py py-obj docutils literal notranslate"><span class="pre">[None]</span></code> for each layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_input_embeddings">
<code class="sig-name descname">get_input_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.nn.modules.module.Module<a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_utils.py#L372-L383"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_input_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model’s input embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch module mapping vocabulary to hidden states.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_output_embeddings">
<code class="sig-name descname">get_output_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.nn.modules.module.Module<a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_utils.py#L398-L405"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.get_output_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model’s output embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch module mapping hidden states to vocabulary.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.half">
<code class="sig-name descname">half</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L344-L350"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.init_weights">
<code class="sig-name descname">init_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_utils.py#L586-L598"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.init_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Initializes and prunes weights if needed.</p>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.invert_attention_mask">
<code class="sig-name descname">invert_attention_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_utils.py#L193-L225"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.invert_attention_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Invert an attention mask (e.g., switches 0. and 1.).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>encoder_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – An attention mask.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The inverted attention mask.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">state_dict</span></em>, <em class="sig-param"><span class="n">strict</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L796-L848"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.state_dict" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.state_dict" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module’s <code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<em>dict</em>) – a dict containing parameters and
persistent buffers.</p></li>
<li><p><strong>strict</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to strictly enforce that the keys
in <a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.state_dict" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this module’s
<code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>missing_keys</strong> is a list of str containing the missing keys</p></li>
<li><p><strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.load_tf_weights">
<code class="sig-name descname">load_tf_weights</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em>, <em class="sig-param"><span class="n">tf_checkpoint_path</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_bert.py#L91-L162"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.load_tf_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Load tf checkpoints in a pytorch model.</p>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.modules">
<code class="sig-name descname">modules</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L990-L1015"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a module in the network</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_buffers">
<code class="sig-name descname">named_buffers</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">prefix</span><span class="o">=</span><span class="default_value">''</span></em>, <em class="sig-param"><span class="n">recurse</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L935-L959"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<em>str</em>) – prefix to prepend to all buffer names.</p></li>
<li><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, torch.Tensor)</em> – Tuple containing the name and buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;running_var&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_children">
<code class="sig-name descname">named_children</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L970-L988"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>(string, Module)</em> – Tuple containing a name and child module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_modules">
<code class="sig-name descname">named_modules</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">memo</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">prefix</span><span class="o">=</span><span class="default_value">''</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L1017-L1053"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>(string, Module)</em> – Tuple of name and module</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_parameters">
<code class="sig-name descname">named_parameters</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">prefix</span><span class="o">=</span><span class="default_value">''</span></em>, <em class="sig-param"><span class="n">recurse</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L887-L911"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.named_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<em>str</em>) – prefix to prepend to all parameter names.</p></li>
<li><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, Parameter)</em> – Tuple containing the name and parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.num_parameters">
<code class="sig-name descname">num_parameters</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">only_trainable</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> &#x2192; int<a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_utils.py#L94-L106"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.num_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the number of (optionally, trainable) parameters in the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>only_trainable</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether or not to return only the number of trainable parameters</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The number of parameters.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.parameters">
<code class="sig-name descname">parameters</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">recurse</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L863-L885"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>Parameter</em> – module parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.prepare_inputs_for_generation">
<code class="sig-name descname">prepare_inputs_for_generation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/generation_utils.py#L36-L41"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.prepare_inputs_for_generation" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement in subclasses of <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code> for custom behavior to prepare inputs in the
generate method.</p>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.prune_heads">
<code class="sig-name descname">prune_heads</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">heads_to_prune</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>int<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_utils.py#L600-L615"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.prune_heads" title="Permalink to this definition">¶</a></dt>
<dd><p>Prunes heads of the base model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>heads_to_prune</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">List[int]]</span></code>) – Dictionary with keys being selected layer indices (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) and associated values being the list
of heads to prune in said layer (list of <code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>). For instance {1: [0, 2], 2: [2, 3]} will
prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_backward_hook">
<code class="sig-name descname">register_backward_hook</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hook</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L445-L476"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> may be tuples if the
module has multiple inputs or outputs. The hook should not modify its
arguments, but it can optionally return a new gradient with respect to
input that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> in subsequent
computations.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The current implementation will not have the presented behavior
for complex <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> that perform many operations.
In some failure cases, <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> will only
contain the gradients for a subset of the inputs and outputs.
For such <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>, you should use <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.register_hook()</span></code>
directly on a specific input or output to get the required gradients.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_buffer">
<code class="sig-name descname">register_buffer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">tensor</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L100-L136"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a persistent buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm’s <code class="docutils literal notranslate"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the persistent state.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the buffer. The buffer can be accessed
from this module using the given name</p></li>
<li><p><strong>tensor</strong> (<em>Tensor</em>) – buffer to be registered.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_forward_hook">
<code class="sig-name descname">register_forward_hook</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hook</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L499-L518"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_forward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after <a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.forward" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> has computed an output.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="n">output</span>
</pre></div>
</div>
<p>The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
<a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.forward" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is called.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_forward_pre_hook">
<code class="sig-name descname">register_forward_pre_hook</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hook</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L478-L497"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_forward_pre_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before <a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.forward" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is invoked.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="nb">input</span>
</pre></div>
</div>
<p>The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_parameter">
<code class="sig-name descname">register_parameter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">param</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L138-L175"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.register_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the parameter. The parameter can be accessed
from this module using the given name</p></li>
<li><p><strong>param</strong> (<em>Parameter</em>) – parameter to be added to the module.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.requires_grad_">
<code class="sig-name descname">requires_grad_</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">requires_grad</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L1090-L1109"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.requires_grad_" title="Permalink to this definition">¶</a></dt>
<dd><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters’ <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>requires_grad</strong> (<em>bool</em>) – whether autograd should record operations on
parameters in this module. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.reset_memory_hooks_state">
<code class="sig-name descname">reset_memory_hooks_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_utils.py#L146-L154"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.reset_memory_hooks_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the <code class="xref py py-obj docutils literal notranslate"><span class="pre">mem_rss_diff</span></code> attribute of each module (see
<code class="xref py py-func docutils literal notranslate"><span class="pre">add_memory_hooks()</span></code>).</p>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.resize_token_embeddings">
<code class="sig-name descname">resize_token_embeddings</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">new_num_tokens</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.nn.modules.sparse.Embedding<a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_utils.py#L509-L537"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.resize_token_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Resizes input token embeddings matrix of the model if <code class="xref py py-obj docutils literal notranslate"><span class="pre">new_num_tokens</span> <span class="pre">!=</span> <span class="pre">config.vocab_size</span></code>.</p>
<p>Takes care of tying weights embeddings afterwards if the model class has a <a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.tie_weights" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.tie_weights"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tie_weights()</span></code></a> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>new_num_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – The number of new tokens in the embedding matrix. Increasing the size will add newly initialized
vectors at the end. Reducing the size will remove vectors from the end. If not provided or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>,
just returns a pointer to the input tokens <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code> module of the model wihtout doing
anything.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Pointer to the input tokens Embeddings Module of the model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.save_pretrained">
<code class="sig-name descname">save_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">save_directory</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_utils.py#L617-L652"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.save_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a model and its configuration file to a directory, so that it can be re-loaded using the
<cite>:func:`~transformers.PreTrainedModel.from_pretrained`</cite> class method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>save_directory</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – Directory to which to save. Will be created if it doesn’t exist.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.set_input_embeddings">
<code class="sig-name descname">set_input_embeddings</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_utils.py#L385-L396"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.set_input_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Set model’s input embeddings</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>) – A module mapping vocabulary to hidden states.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.state_dict">
<code class="sig-name descname">state_dict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">destination</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">prefix</span><span class="o">=</span><span class="default_value">''</span></em>, <em class="sig-param"><span class="n">keep_vars</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L681-L709"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a dictionary containing a whole state of the module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.tie_weights">
<code class="sig-name descname">tie_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/transformers/modeling_utils.py#L407-L419"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.tie_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Tie the weights between the input embeddings and the output embeddings.</p>
<p>If the <code class="xref py py-obj docutils literal notranslate"><span class="pre">torchscript</span></code> flag is set in the configuration, can’t handle parameter sharing so we are cloning
the weights instead.</p>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.to">
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L360-L443"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as</p>
<dl class="py function">
<dt id="id0">
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">device</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">non_blocking</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L360-L443"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#id0" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="id1">
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dtype</span></em>, <em class="sig-param"><span class="n">non_blocking</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L360-L443"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#id1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="id2">
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">non_blocking</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L360-L443"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#id2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="id3">
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">memory_format</span><span class="o">=</span><span class="default_value">torch.channels_last</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L360-L443"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#id3" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Its signature is similar to <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code>, but only accepts
floating point desired <a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.dtype" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a> s. In addition, this method will
only cast the floating point parameters and buffers to <a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.dtype" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a>
(if given). The integral parameters and buffers will be moved
<a class="reference internal" href="../_autosummary_backup/transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.html#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.device" title="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>) – the desired device of the parameters
and buffers in this module</p></li>
<li><p><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>) – the desired floating point type of
the floating point parameters and buffers in this module</p></li>
<li><p><strong>tensor</strong> (<em>torch.Tensor</em>) – Tensor whose dtype and device are the desired
dtype and device for all parameters and buffers in this module</p></li>
<li><p><strong>memory_format</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>) – the desired memory
format for 4D parameters and buffers in this module (keyword
only argument)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gpu1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">gpu1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.train">
<code class="sig-name descname">train</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">mode</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L1055-L1073"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mode</strong> (<em>bool</em>) – whether to set training mode (<code class="docutils literal notranslate"><span class="pre">True</span></code>) or evaluation
mode (<code class="docutils literal notranslate"><span class="pre">False</span></code>). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.type">
<code class="sig-name descname">type</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dst_type</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L317-L326"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dst_type</strong> (<em>type</em><em> or </em><em>string</em>) – the desired type</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.zero_grad">
<code class="sig-name descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/Guzpenha/transformer_rankers/tree//master/env/lib/python3.6/site-packages/torch/nn/modules/module.py#L1111-L1116"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformer_rankers.models.pairwise_bert.BertForPairwiseLearning.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets gradients of all model parameters to zero.</p>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="transformer_rankers.models.pointwise_bert.html" class="btn btn-neutral float-right" title="transformer_rankers.models.pointwise_bert" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="transformer_rankers.models.pairwise_bert.html" class="btn btn-neutral float-left" title="transformer_rankers.models.pairwise_bert" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Gustavo Penha

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search .wy-dropdown>a, .wy-side-nav-search>a {
        color: #A43025;
    }
    .wy-side-nav-search, .wy-nav-top {
      background-color: #ffffff;      
    }
    /* Sidebar */
    .wy-nav-side {
      background: #3F3F3F;
    }
  </style>


</body>
</html>